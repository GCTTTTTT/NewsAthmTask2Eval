{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90e660f7-b749-4889-a07e-5baa6e5759d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 结果还行！！！！！初版完成！！！！\n",
    "# FIX：3.25 归一化后是乘以100！\n",
    "# Fix:3.29 mbert+bilstm+SelfAttention版本\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "275155fa-9e42-4e18-82f9-da7672e9d289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义Self-Attention层\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    # 定义模型\n",
    "class NewsClassifier(nn.Module):\n",
    "    # hidden_size = 128\n",
    "    def __init__(self, bert_model,num_classes, hidden_size, num_layers=2, bidirectional=True):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "        self.bert = bert_model # FIX\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "        #                     bidirectional=bidirectional, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True) # FIX\n",
    "        \n",
    "        self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "        attention_outputs = self.attention(lstm_outputs)\n",
    "        logits = self.fc(attention_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9894373e-f965-4428-945a-3d7946438f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载训练好的模型\n",
    "model_path = '../NewsAthm/bert-base-multilingual-cased'  ## 可更换\n",
    "# modelNew_load_path = './classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "# modelNew_load_path = '../NewsAthmTask2Score/classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "modelNew_load_path = '../NewsAthmTask2Score/classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "\n",
    "model_CLS_name = \"mbert_BiLSTM_SelfAttention\" ###!!!\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path, num_labels=9)\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 将模型移动到GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aba5bbe4-6917-4cff-b45b-ff8173b4c4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# 设置超参数\n",
    "max_length = 512\n",
    "hidden_size = 128\n",
    "num_classes = 9\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91d0384c-e95a-4c19-9838-5a1907bd42e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载训练好的模型\n",
    "# modelNew_load_path = '../NewsAthmTask2Score/classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "model = NewsClassifier(bert_model = model,num_classes=num_classes, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "model.load_state_dict(torch.load(modelNew_load_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 定义类别列表\n",
    "# categories = ['খেলাধুলা', 'রাজনীতি', 'বিনোদন', 'অর্থনীতি', 'আইন', 'শিক্ষা', 'বিজ্ঞান', 'লাইফস্টাইল', 'অন্যান্য']\n",
    "categories = ['রাজনীত','লাইফস্টাইল','শিক্ষা','অর্থনীতি','খেলাধুলা','অন্যান্য','বিজ্ঞান','বিনোদন', 'আইন'] # FIX!!!!!!!!!!!!\n",
    "# [‘政治’、‘生活方式’、‘教育’、‘经济’、‘体育’、‘其他’、‘科学’、‘娱乐’、‘法律’]\n",
    "\n",
    "# 定义数据处理函数\n",
    "def preprocess_data(text, tokenizer, max_length):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    return encoding['input_ids'].to(device), encoding['attention_mask'].to(device)\n",
    "\n",
    "# 定义预测函数\n",
    "def predict(model, input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return preds.item(), categories[preds.item()]\n",
    "    # return preds.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f73a569b-83b6-4ac1-a90f-50e80c51ad7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 读取csv文件\n",
    "# data = pd.read_csv('./Data231202-231211/Data231202.csv')  ## \n",
    "# data = pd.read_csv('./datasets/news_20240302_20240311.csv')  ## 对0302-0311这10天进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "data = pd.read_csv('./datasets/news_20240302_20240318.csv')  ## 对0302-0318这进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "date_UNI = '2024-03-12' ###\n",
    "# 筛选 pub_time 为 '2024-03-02' 的数据\n",
    "filtered_data = data[data['pub_time'] == date_UNI]  ## 这个日期是参数！系统端传过来后进行处理 系统传日期---》查询数据库（看是否有缓存。没有的话就现查）---》筛选\n",
    "\n",
    "# filtered_data.to_csv(\"./test0302.csv\", index=False)\n",
    "\n",
    "# 显示筛选结果\n",
    "# print(filtered_data)\n",
    "\n",
    "\n",
    "nan_check = filtered_data['body'].isna().sum()\n",
    "nan_check_c = filtered_data['category1'].isna().sum()\n",
    "print(nan_check)\n",
    "print(nan_check_c)\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['category1','body'])\n",
    "nan_check = filtered_data['body'].isna().sum()\n",
    "nan_check_c = filtered_data['category1'].isna().sum()\n",
    "print(nan_check)\n",
    "print(nan_check_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1bbba4ac-6e8e-4459-8f6b-6eba8923cb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>website_id</th>\n",
       "      <th>request_url</th>\n",
       "      <th>response_url</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>cole_time</th>\n",
       "      <th>images</th>\n",
       "      <th>language_id</th>\n",
       "      <th>md5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>20044678</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=101264</td>\n",
       "      <td>https://mzamin.com/news.php?news=101264</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>শাকিবের নায়িকা হচ্ছেন মিমি-নাবিলা</td>\n",
       "      <td>‘তুফান’ সিনেমায় রায়হান রাফীর পরিচালনায় প্রথমবা...</td>\n",
       "      <td>‘তুফান’ সিনেমায় রায়হান রাফীর পরিচালনায় প্রথমবা...</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>2024-03-12 07:57:18</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/101264_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>15e008acdc188d0a146da231cbfc2ab2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>20044679</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=101263</td>\n",
       "      <td>https://mzamin.com/news.php?news=101263</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>৯৬তম অস্কার পেলেন যারা-</td>\n",
       "      <td>চলচ্চিত্র দুনিয়ার সবচেয়ে মর্যাদাপূর্ণ আয়োজন অস...</td>\n",
       "      <td>চলচ্চিত্র দুনিয়ার সবচেয়ে মর্যাদাপূর্ণ আয়োজন অস...</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>2024-03-12 07:57:19</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/101263_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>415ce7ac5b9738cb31da49347cd31e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>20044680</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=101261</td>\n",
       "      <td>https://mzamin.com/news.php?news=101261</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>সেরা অভিনেতা হলেন শাহরুখ</td>\n",
       "      <td>বলিউডের সম্মানজনক পুরস্কারগুলোর একটি ‘জি সিনে ...</td>\n",
       "      <td>বলিউডের সম্মানজনক পুরস্কারগুলোর একটি ‘জি সিনে ...</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>2024-03-12 07:57:20</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/101261_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>3687c9a64c9383cdf89c178c912a5723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>20044681</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=101260</td>\n",
       "      <td>https://mzamin.com/news.php?news=101260</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>বঙ্গবন্ধুকে নিয়ে গাইলেন ইভা</td>\n",
       "      <td>জাতির জনক বঙ্গবন্ধু শেখ মুজিবুর রহমানকে নিয়ে এ...</td>\n",
       "      <td>জাতির জনক বঙ্গবন্ধু শেখ মুজিবুর রহমানকে নিয়ে এ...</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>2024-03-12 07:57:21</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/101260_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>72e5274bc5f4d8e843a2f8582ed1bf8a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>20044682</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=101259</td>\n",
       "      <td>https://mzamin.com/news.php?news=101259</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>বিয়ে করতে গিয়ে গ্রেপ্তার</td>\n",
       "      <td>বিগ বস খ্যাত অভিনেত্রী সোফিয়া হায়াত সোশ্যাল মি...</td>\n",
       "      <td>বিগ বস খ্যাত অভিনেত্রী সোফিয়া হায়াত সোশ্যাল মি...</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>2024-03-12 07:57:21</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/101259_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>dff9a2fa6fe2d22d389ee9449dfbe939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  website_id                              request_url  \\\n",
       "3067  20044678        2268  https://mzamin.com/news.php?news=101264   \n",
       "3068  20044679        2268  https://mzamin.com/news.php?news=101263   \n",
       "3069  20044680        2268  https://mzamin.com/news.php?news=101261   \n",
       "3070  20044681        2268  https://mzamin.com/news.php?news=101260   \n",
       "3071  20044682        2268  https://mzamin.com/news.php?news=101259   \n",
       "\n",
       "                                 response_url category1 category2  \\\n",
       "3067  https://mzamin.com/news.php?news=101264     সংবাদ       NaN   \n",
       "3068  https://mzamin.com/news.php?news=101263     সংবাদ       NaN   \n",
       "3069  https://mzamin.com/news.php?news=101261     সংবাদ       NaN   \n",
       "3070  https://mzamin.com/news.php?news=101260     সংবাদ       NaN   \n",
       "3071  https://mzamin.com/news.php?news=101259     সংবাদ       NaN   \n",
       "\n",
       "                                  title  \\\n",
       "3067  শাকিবের নায়িকা হচ্ছেন মিমি-নাবিলা   \n",
       "3068            ৯৬তম অস্কার পেলেন যারা-   \n",
       "3069           সেরা অভিনেতা হলেন শাহরুখ   \n",
       "3070        বঙ্গবন্ধুকে নিয়ে গাইলেন ইভা   \n",
       "3071           বিয়ে করতে গিয়ে গ্রেপ্তার   \n",
       "\n",
       "                                               abstract  \\\n",
       "3067  ‘তুফান’ সিনেমায় রায়হান রাফীর পরিচালনায় প্রথমবা...   \n",
       "3068  চলচ্চিত্র দুনিয়ার সবচেয়ে মর্যাদাপূর্ণ আয়োজন অস...   \n",
       "3069  বলিউডের সম্মানজনক পুরস্কারগুলোর একটি ‘জি সিনে ...   \n",
       "3070  জাতির জনক বঙ্গবন্ধু শেখ মুজিবুর রহমানকে নিয়ে এ...   \n",
       "3071  বিগ বস খ্যাত অভিনেত্রী সোফিয়া হায়াত সোশ্যাল মি...   \n",
       "\n",
       "                                                   body   pub_time  \\\n",
       "3067  ‘তুফান’ সিনেমায় রায়হান রাফীর পরিচালনায় প্রথমবা... 2024-03-12   \n",
       "3068  চলচ্চিত্র দুনিয়ার সবচেয়ে মর্যাদাপূর্ণ আয়োজন অস... 2024-03-12   \n",
       "3069  বলিউডের সম্মানজনক পুরস্কারগুলোর একটি ‘জি সিনে ... 2024-03-12   \n",
       "3070  জাতির জনক বঙ্গবন্ধু শেখ মুজিবুর রহমানকে নিয়ে এ... 2024-03-12   \n",
       "3071  বিগ বস খ্যাত অভিনেত্রী সোফিয়া হায়াত সোশ্যাল মি... 2024-03-12   \n",
       "\n",
       "                cole_time                                             images  \\\n",
       "3067  2024-03-12 07:57:18  [\"https://mzamin.com/uploads/news/main/101264_...   \n",
       "3068  2024-03-12 07:57:19  [\"https://mzamin.com/uploads/news/main/101263_...   \n",
       "3069  2024-03-12 07:57:20  [\"https://mzamin.com/uploads/news/main/101261_...   \n",
       "3070  2024-03-12 07:57:21  [\"https://mzamin.com/uploads/news/main/101260_...   \n",
       "3071  2024-03-12 07:57:21  [\"https://mzamin.com/uploads/news/main/101259_...   \n",
       "\n",
       "      language_id                               md5  \n",
       "3067         1779  15e008acdc188d0a146da231cbfc2ab2  \n",
       "3068         1779  415ce7ac5b9738cb31da49347cd31e6b  \n",
       "3069         1779  3687c9a64c9383cdf89c178c912a5723  \n",
       "3070         1779  72e5274bc5f4d8e843a2f8582ed1bf8a  \n",
       "3071         1779  dff9a2fa6fe2d22d389ee9449dfbe939  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aef5fd3b-23da-4610-9d40-47d12a221934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH!!\n"
     ]
    }
   ],
   "source": [
    "processed_data_file_name = f\"./datasets/{model_CLS_name}/news_{date_UNI}_processed_{model_CLS_name}.csv\"\n",
    "# FIX:缓存操作 若已有文件则直接读取 否则才进行预测\n",
    "# 判断文件是否存在\n",
    "if os.path.exists(processed_data_file_name):\n",
    "    # 如果文件存在，则直接读取数据\n",
    "    processed_data = pd.read_csv(processed_data_file_name)\n",
    "else:\n",
    "    # 如果文件不存在，则执行处理数据的函数\n",
    "    # processed_data = process_data(data)\n",
    "    # processed_data = process_data(filtered_data)\n",
    "    \n",
    "    # FIX:\n",
    "    predicted_categories = []\n",
    "    cnt = 0\n",
    "    for idx, row in filtered_data.iterrows():\n",
    "        cnt+=1\n",
    "        if cnt%200 == 0: \n",
    "            print(cnt)\n",
    "        if row['category1'] not in categories:\n",
    "            input_ids, attention_mask = preprocess_data(row['body'], tokenizer, max_length)\n",
    "            pred_id, predicted_category = predict(model, input_ids, attention_mask)\n",
    "            predicted_categories.append(predicted_category)\n",
    "            # predicted_categories_id.append(pred_id)\n",
    "        else:\n",
    "            predicted_categories.append(row['category1'])\n",
    "            # predicted_categories_id.append(row['category1'])\n",
    "            \n",
    "    # 将预测后的类别替换原有的category1列\n",
    "    filtered_data['category1'] = predicted_categories\n",
    "    processed_data = filtered_data\n",
    "    \n",
    "    # 将处理后的数据保存到文件中\n",
    "    processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "\n",
    "# # 处理数据\n",
    "# processed_data = process_data(data)\n",
    "\n",
    "# # 保存处理后的数据到新的csv文件\n",
    "# # processed_data.to_csv('./Data231202-231211/Data231202_processed.csv', index=False)\n",
    "# processed_data_file_name = f\"./datasets/news_{date_UNI}_processed.csv\"\n",
    "# processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "\n",
    "print(\"FINISH!!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "91952c79-c679-4ea4-8393-999ecaf297fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# conda angle https://github.com/SeanLee97/AnglE/tree/main\n",
    "# pip install nltk\n",
    "# pip install --upgrade pip\n",
    "# pip install spacy==2.3.5\n",
    "# pip install bn_core_news_sm-0.1.0.tar.gz\n",
    "# pip install matplotlib\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "# yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "# update:单个成簇的处理\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize # 使用NLTK进行分词，根据需要替换为适合孟加拉语的分词方法\n",
    "\n",
    "import spacy\n",
    "# from gensim.summarization import keywords\n",
    "from collections import defaultdict\n",
    "import bn_core_news_sm\n",
    "from sklearn.preprocessing import MinMaxScaler # 归一化\n",
    "import matplotlib.pyplot as plt\n",
    "# import pytextrank\n",
    "# =======\n",
    "# 去除停用词\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import string\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75be79-c687-42f6-932f-84ce1a97ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "18535716-78bc-40ae-b425-97ec8e44752e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10, 105], [11, 133], [12, 34], [13, 50], [14], [15], [16, 49], [17, 290], [18], [19], [20], [21], [22], [23, 250], [24, 207], [25], [26, 272], [27, 54], [28, 127], [29, 60], [30], [31], [32], [33], [35], [36, 37], [38, 166], [39, 72], [40], [41], [42], [43], [44, 206], [45], [46], [47], [48, 412], [51], [52, 328], [53], [55], [56], [57], [58], [59], [61], [62], [63], [64], [65, 209], [66], [67, 397], [68, 71], [69, 85], [70], [73, 158], [74], [75], [76], [77], [78], [79], [80], [81, 191], [82], [83, 90], [84], [86], [87], [88, 392], [89, 293], [91], [92], [93], [94], [95], [96], [97], [98, 147], [99], [100, 168], [101, 215], [102], [103, 114], [104], [106, 401], [107, 151], [108], [109, 291], [110], [111], [112, 197], [113], [115, 167], [116, 194], [117], [118], [119, 256], [120, 313], [121, 299], [122], [123], [124], [125], [126, 448], [128], [129], [130], [131], [132], [134], [135, 139], [136, 282], [137, 279], [138], [140], [141], [142, 234], [143], [144], [145], [146], [148], [149, 337], [150], [152], [153, 398], [154, 353], [155], [156], [157, 275], [159], [160], [161], [162], [163], [164], [165], [169], [170, 172], [171], [173], [174, 227], [175, 310], [176], [177, 259], [178], [179], [180], [181, 184], [182], [183], [185], [186], [187], [188], [189, 305], [190], [192], [193], [195], [196], [198, 199], [200], [201, 348], [202], [203, 292], [204], [205], [208], [210], [211], [212], [213], [214], [216], [217], [218, 342], [219, 240], [220, 244], [221, 360], [222, 440], [223, 382], [224, 339], [225], [226], [228, 248], [229], [230], [231], [232, 238], [233, 241], [235, 281], [236], [237, 262], [239], [242], [243, 351], [245], [246, 251], [247], [249], [252], [253], [254], [255], [257], [258, 421], [260, 355], [261, 298], [263, 393], [264], [265], [266], [267], [268], [269, 365], [270, 410], [271], [273], [274], [276], [277], [278], [280, 359], [283], [284, 336], [285], [286], [287], [288], [289], [294], [295, 347], [296], [297], [300], [301], [302], [303], [304], [306], [307], [308], [309], [311], [312], [314], [315], [316], [317], [318], [319, 404], [320, 442], [321], [322], [323], [324], [325], [326], [327], [329], [330], [331, 367], [332], [333], [334, 361], [335, 369], [338], [340, 434], [341], [343], [344], [345], [346], [349], [350], [352], [354], [356, 357], [358], [362], [363], [364], [366], [368], [370], [371, 445], [372], [373, 378], [374], [375], [376], [377], [379, 383], [380], [381, 436], [384], [385], [386], [387], [388], [389, 435], [390], [391], [394, 407], [395], [396], [399], [400], [402], [403], [405], [406], [408], [409], [411], [413], [414], [415], [416], [417], [418], [419], [420], [422], [423], [424], [425], [426], [427], [428], [429, 443], [430], [431], [432], [433], [437], [438], [439, 441], [444], [446], [447], [449], [450], [451], [452], [453]]\n",
      "FINISH!\n"
     ]
    }
   ],
   "source": [
    "# data_ORI = pd.read_csv('./Data231202-231211/Data231202.csv') # 所有子任务都是使用这个\n",
    "data_ORI = processed_data\n",
    "\n",
    "# 使用angle加载\n",
    "model_id = '../NewsAthmTask2/models/angle-bert-base-uncased-nli-en-v1' ## 可更换\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "\n",
    "# 加载数据\n",
    "data = data_ORI\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "     \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "def get_predicted_clusters(data,threshold):\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "        # 获取该日期的新闻正文\n",
    "        # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "        embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "\n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "    \n",
    "    return predicted_clusters\n",
    "\n",
    "# 设置阈值\n",
    "threshold = 0.972  ## 可更换\n",
    "clusters = get_predicted_clusters(data,threshold)\n",
    "\n",
    "# 创建一个字典，键是语料索引，值是对应的簇大小\n",
    "index_to_cluster_size = {index: len(cluster) for cluster in clusters for index in cluster}\n",
    "\n",
    "# 读取语料文件\n",
    "df = data_ORI\n",
    "\n",
    "# 新增列clus_news_num，记录每个语料对应的簇的大小\n",
    "df['T1_clus_news_num'] = df.index.map(index_to_cluster_size)\n",
    "\n",
    "# 根据簇大小进行排序，并添加排名，相同大小的排名相同\n",
    "df = df.sort_values(by='T1_clus_news_num', ascending=False)\n",
    "df['T1_rank'] = df['T1_clus_news_num'].rank(method='min', ascending=False)\n",
    "\n",
    "# 新增列S_scale，为簇大小的归一化结果\n",
    "scaler = MinMaxScaler()\n",
    "df['T1_S_scale'] = scaler.fit_transform(df[['T1_clus_news_num']])\n",
    "\n",
    "# # 新增列S_score，为S_scale的值乘以20\n",
    "# df['T1_S_score'] = df['T1_S_scale'] * 20\n",
    "\n",
    "# 新增列S_score，为S_scale的值乘以20\n",
    "df['T1_S_score'] = df['T1_S_scale'] * 100\n",
    "\n",
    "# 新增列index，表示语料原始的坐标\n",
    "df['T1_ori_indexFrom0'] = df.index\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T1_final_df = df[['id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score']]\n",
    "\n",
    "# 文件保存处理，若有重名文件，则重命名为_{num}  好像并不需要 每天的是固定的 后续可能直接查询就行\n",
    "# num_file_T1 = 1\n",
    "\n",
    "# # 检查文件是否存在\n",
    "# while os.path.exists(T1_file_name):\n",
    "#     T1_file_name = f\"./T1ClusterScore/T1_{date_UNI}_result_new_{num_file_T1}.csv\"\n",
    "#     num_file_T1 += 1\n",
    "\n",
    "T1_file_name = f\"./T1ClusterScore/{model_CLS_name}/T1_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "T1_final_df.to_csv(T1_file_name, index=False)\n",
    "print(\"FINISH!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "de71bd00-2306-4c71-b2d3-58eb1c704ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 40个网站的排名以及赋分结果在./T2WebsiteRank/website_Rank_new.csv\n",
    "# Data231202-231211/Data231202.csv\n",
    "# 读取Data231202-231211/Data231202.csv，其中的website_id为网站id，现在读取./T2WebsiteRank/website_Rank_new.csv，该文件存有website_id对应的S_task_web，现在需要将Data231202.csv中的每个语料对应的website_id对应的S_task_web新增一列进行存储，然后根据S_task_web进行排序，允许并列，新增rank列，将结果中website_id,title,S_task_web,rank存到新的csv文件\n",
    "\n",
    "# 读取两个csv文件\n",
    "data_df = data_ORI\n",
    "# rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new.csv')\n",
    "rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new_FIX.csv') # FIX\n",
    "\n",
    "\n",
    "# 将两个DataFrame合并\n",
    "merged_df = pd.merge(data_df, rank_df, on='website_id')\n",
    "\n",
    "# 根据S_task_web列进行排序，并添加排名，相同权重的排名相同\n",
    "merged_df = merged_df.sort_values(by='T2_S_score', ascending=False)\n",
    "merged_df['T2_rank'] = merged_df['T2_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T2_final_df = merged_df[['id','website_id', 'title', 'T2_S_score', 'T2_rank']]\n",
    "\n",
    "T2_file_name = f\"./T2WebsiteRank/{model_CLS_name}/T2_{date_UNI}_{model_CLS_name}_result_new.csv\" ## FIX\n",
    "# T2_final_df.to_csv('./T2WebsiteRank/Data231202_scoreResult.csv', index=False)\n",
    "T2_final_df.to_csv(T2_file_name, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "01c77133-66c4-418c-b163-4a98f93c7512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，并将结果保存到新的CSV文件中。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取CSV文件并计算正文长度\n",
    "df = data_ORI\n",
    "df['body_len'] = df['body'].apply(lambda x: len(str(x).split()))  # 假设每个单词之间用空格分隔\n",
    "\n",
    "# 按正文长度进行排序\n",
    "df = df.sort_values(by='body_len', ascending=False)\n",
    "\n",
    "# 添加排名列\n",
    "df['T3_rank'] = df['body_len'].rank(method='min', ascending=False)\n",
    "\n",
    "# 计算S_scale并添加列\n",
    "max_len = df['body_len'].max()\n",
    "min_len = df['body_len'].min()\n",
    "df['T3_S_scale'] = (df['body_len'] - min_len) / (max_len - min_len)\n",
    "\n",
    "# # 计算body_len_score并添加列\n",
    "# df['T3_S_score'] = 20 * df['T3_S_scale']\n",
    "\n",
    "# 计算body_len_score并添加列\n",
    "df['T3_S_score'] = 100 * df['T3_S_scale'] #FIX\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "T3_file_name_1 = f\"./T3BodyLenRank/{model_CLS_name}/T3_{date_UNI}_{model_CLS_name}_result_new_all.csv\"\n",
    "T3_file_name_2 = f\"./T3BodyLenRank/{model_CLS_name}/T3_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "\n",
    "# output_file = './T3BodyLenRank/Data231202_newDATA_rank_Score_new.csv'  # 替换为你的输出文件路径\n",
    "# df.to_csv(output_file, index=False)\n",
    "df.to_csv(T3_file_name_1, index=False)\n",
    "\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T3_final_df = df[['id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score']]\n",
    "# T3_final_df.to_csv('./T3BodyLenRank/Data231202_T3scoreResult.csv', index=False)\n",
    "T3_final_df.to_csv(T3_file_name_2, index=False)\n",
    "\n",
    "print(\"处理完成，并将结果保存到新的CSV文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6698c061-4cca-438a-827c-7c8a869f8dca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’\n",
      "<class 'list'>\n",
      "filtered_titles len\n",
      " 2861\n",
      "20512\n",
      "2861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载孟加拉语模型\n",
    "nlp = bn_core_news_sm.load()\n",
    "# # textrank算法计算权重\n",
    "# update 3.9：改进版！！\n",
    "def textrank_weighted_word_graph(merged_titles):\n",
    "    tokens = nlp(merged_titles) # 分词\n",
    "    print(len(tokens))\n",
    "    # print(tokens)\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    window_size = 80  # 根据需要调整窗口大小\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        for j in range(i+1, min(i+window_size+1, len(tokens))):\n",
    "            if token != tokens[j]:  # 添加边,避免自环\n",
    "                if graph.has_edge(token, tokens[j]):\n",
    "                    graph[token][tokens[j]]['weight'] += 1 #在添加边时,先检查边是否已经存在。如果边已经存在,则将权重加1;否则,添加一个新边,权重为1。这样可以避免重复添加边。\n",
    "                else:\n",
    "                    graph.add_edge(token, tokens[j], weight=1)\n",
    "    \n",
    "    # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "    pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    return pagerank_scores,graph\n",
    "\n",
    "# 读取CSV文件并合并所有标题\n",
    "df = data_ORI\n",
    "\n",
    "merged_titles = ' '.join(title.strip() for title in df['title'])\n",
    "\n",
    "# ====================================\n",
    "# 获取孟加拉语的停用词列表\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "# print(stop_words)\n",
    "\n",
    "# 自定义标点符号列表\n",
    "custom_punctuation = ['‘', '’']\n",
    "\n",
    "# 合并 NLTK 提供的标点符号列表和自定义标点符号列表\n",
    "all_punctuation = string.punctuation + ''.join(custom_punctuation)\n",
    "\n",
    "print(all_punctuation)\n",
    "# 分词# word_tokens = word_tokenize(merged_titles)\n",
    "\n",
    "word_tokens = nlp(merged_titles) # 分词\n",
    "# word_tokens = merged_titles.split() # 根据空格分词\n",
    "token_texts = [token.text.strip() for token in word_tokens] # 去除多余空格\n",
    "\n",
    "# print(token_texts)\n",
    "print(type(token_texts))\n",
    "\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "# filtered_titles = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_titles = [w for w in token_texts if not w in stop_words] # 去除停用词\n",
    "filtered_titles = [word for word in filtered_titles if word not in all_punctuation] # 去除标点符号\n",
    "\n",
    "print(\"filtered_titles len\\n\",len(filtered_titles)) # 字符串数量！\n",
    "\n",
    "# 将去除停用词后的词重新组合成字符串\n",
    "filtered_titles_text = ' '.join(filtered_titles)\n",
    "\n",
    "print(len(filtered_titles_text)) # 字符串长度！别被误导（所少个字符）\n",
    "# ====================================\n",
    "\n",
    "# 计算词权重\n",
    "word_weights,graph = textrank_weighted_word_graph(filtered_titles_text)\n",
    "\n",
    "# 保存pagerank算法后的词关系权重 可视化\n",
    "# 根据PageRank值更新边的权重\n",
    "# 记录权重关系 字典形式存储\n",
    "pagerank_weighted_graph = nx.Graph()\n",
    "for node, score in word_weights.items():\n",
    "    pagerank_weighted_graph.add_node(node)\n",
    "\n",
    "for u, v, data in graph.edges(data=True):\n",
    "    weight = data['weight'] * word_weights[u] * word_weights[v]\n",
    "    pagerank_weighted_graph.add_edge(u, v, weight=weight)\n",
    "\n",
    "graph_content_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_graph_content.txt\"\n",
    "with open('./T4TitleTextRank/graph_content.txt', 'w') as file:\n",
    "    file.write(str(nx.to_dict_of_dicts(pagerank_weighted_graph)))\n",
    "\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "# word_weights_df = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "word_weights_df = pd.DataFrame(sorted_words, columns=['word', 'weight'])\n",
    "\n",
    "word_weight_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_word_weight_new.csv\"\n",
    "\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight.csv', index=False)\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight_new.csv', index=False)\n",
    "word_weights_df.to_csv(word_weight_file_name, index=False)\n",
    "\n",
    "# 接下来，计算每个标题的权重\n",
    "# 读取词权重文件\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight.csv')\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight_new.csv')\n",
    "word_weights_df = pd.read_csv(word_weight_file_name)\n",
    "\n",
    "\n",
    "# 将词权重转换为字典，方便查找\n",
    "word_weights = pd.Series(word_weights_df.weight.values, index=word_weights_df.word).to_dict()\n",
    "\n",
    "# print(word_weights)\n",
    "# 读取新闻标题文件\n",
    "titles_df = data_ORI\n",
    "# titles_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "# titles_df = titles_df['title']\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，用于计算标题的权重\n",
    "def calculate_title_weight(title):\n",
    "    doc = nlp(title)\n",
    "    # 对标题进行分词并计算总权重\n",
    "    return sum(word_weights.get(token.text, 0) for token in doc)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in all_punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in string.punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "\n",
    "\n",
    "# 计算每个标题的权重\n",
    "titles_df['T4_title_weight'] = titles_df['title'].apply(calculate_title_weight)\n",
    "# print(titles_df['T4_title_weight'])\n",
    "\n",
    "# 根据权重排序并添加排名，相同权重的排名相同\n",
    "titles_df = titles_df.sort_values(by='T4_title_weight', ascending=False)\n",
    "titles_df['T4_rank'] = titles_df['T4_title_weight'].rank(method='min', ascending=False)\n",
    "\n",
    "# 对权重进行归一化处理，并存储结果到\"S_scale\"列\n",
    "scaler = MinMaxScaler()\n",
    "titles_df['T4_S_scale'] = scaler.fit_transform(titles_df[['T4_title_weight']])  # 归一化映射到分数！\n",
    "\n",
    "# # 创建\"S_score\"列\n",
    "# titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 20\n",
    "\n",
    "# 创建\"S_score\"列\n",
    "titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 100\n",
    "\n",
    "# 只保留需要的列\n",
    "T4_final_df = titles_df[['id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score']]\n",
    "\n",
    "\n",
    "# 保存到新的csv文件\n",
    "# final_df.to_csv('./T4TitleTextRank/titles_weight.csv', index=False)\n",
    "\n",
    "T4_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "# T4_final_df.to_csv('./T4TitleTextRank/titles_weight_new.csv', index=False)\n",
    "T4_final_df.to_csv(T4_file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a042abc9-01b0-4f8a-ac4f-a49ee2f6a9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 提取新闻的category1进行类别评分\n",
    "\n",
    "# category_df = pd.read_csv('./T5CateforyScore/category_score.csv')\n",
    "category_df = pd.read_csv('./T5CateforyScore/category_score_FIX.csv')\n",
    "\n",
    "\n",
    "# Load the CSV file with news data\n",
    "# news_df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "news_df = data_ORI\n",
    "\n",
    "\n",
    "# Merge the two DataFrames based on the \"category1\" column\n",
    "merged_df = pd.merge(news_df, category_df, how='left', left_on='category1', right_on='category')\n",
    "\n",
    "# Sort the merged DataFrame based on the \"rank\" column\n",
    "sorted_df = merged_df.sort_values(by='T5_rank')\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = ['id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score']\n",
    "T5_final_df = sorted_df[selected_columns]\n",
    "\n",
    "T5_file_name = f\"./T5CateforyScore/{model_CLS_name}/T5_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "# Save the result to a new CSV file\n",
    "# T5_final_df.to_csv('./T5CateforyScore/Data231202_categoryScore_new.csv', index=False)\n",
    "T5_final_df.to_csv(T5_file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f34e23de-6f51-4a51-a86c-1912cf58520f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# T1_final_df :'id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score'\n",
    "# T2_final_df:'id','website_id', 'title', 'T2_S_score', 'T2_rank'\n",
    "# T3_final_df:'id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score'\n",
    "# T4_final_df: 'id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score'\n",
    "# T5_final_df:'id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score'\n",
    "# 合并5个dataframe：\n",
    "# 第一步:将T1_final_df和T2_final_df合并\n",
    "merged_df = pd.merge(T1_final_df, T2_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第二步:将第一步合并后的DataFrame与T3_final_df合并\n",
    "merged_df = pd.merge(merged_df, T3_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第三步:将第二步合并后的DataFrame与T4_final_df合并\n",
    "merged_df = pd.merge(merged_df, T4_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第四步:将第三步合并后的DataFrame与T5_final_df合并\n",
    "merged_df = pd.merge(merged_df, T5_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 打印合并后的 DataFrame\n",
    "Merge_file_name = f\"./MergeFiveDScore/{model_CLS_name}/Merge_{date_UNI}_{model_CLS_name}_FiveDScore_result_new.csv\"\n",
    "# merged_df.to_csv('./MergeFiveDScore/FiveDScore_Merge.csv', index=False)\n",
    "merged_df.to_csv(Merge_file_name, index=False)\n",
    "\n",
    "# print(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1849acb-8245-462f-bc4c-b3e1fde35554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 假设权重 \n",
    "# w1, w2, w3, w4, w5 = 0.5,0.05,0.05,0.3,0.1\n",
    "# 权重设置思路：\n",
    "# ①层次分析法 根据各任务的重要性赋权\n",
    "# ②迭代 需要一个评估指标（正确个数？）来进行迭代找出模型最优权重！\n",
    "\n",
    "# 层次分析法权重！：\n",
    "# 通过进行层次分析法确定的五个维度权重为:相似新闻报道频率(0.46221)、新闻来源网站权威性(0.03503)、新闻标题重要性(0.35029)、新闻正文长度(0.03049)、新闻类别(0.12198)。\n",
    "# 对应\n",
    "# T1：0.46221 相似新闻---clusterScore\n",
    "# T2: 0.03503 网站权威性---WebsiteRank \n",
    "# T3：0.03049 正文长度---bodyLenRank\n",
    "# T4：0.35029 新闻标题重要性 --- TitleTextRank\n",
    "# T5：0.12198 新闻类别 --- Category\n",
    "\n",
    "w1, w2, w3, w4, w5 = 0.46221, 0.03503, 0.03049, 0.35029, 0.12198\n",
    "\n",
    "\n",
    "# 计算总分数\n",
    "merged_df['total_S_score'] = w1 * merged_df['T1_S_score'] + w2 * merged_df['T2_S_score'] + w3 * merged_df['T3_S_score'] + w4 * merged_df['T4_S_score'] + w5 * merged_df['T5_S_score']\n",
    "\n",
    "# 生成排名\n",
    "merged_df['total_rank'] = merged_df['total_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 根据总分数降序排序\n",
    "merged_df = merged_df.sort_values('total_S_score', ascending=False)\n",
    "\n",
    "# 将结果保存到csv文件\n",
    "total_result_file_name = f\"./MergeFiveDScore/{model_CLS_name}/total_result_{date_UNI}_{model_CLS_name}.csv\"\n",
    "# merged_df.to_csv('./MergeFiveDScore/total_result.csv', index=False)\n",
    "merged_df.to_csv(total_result_file_name , index=False)\n",
    "\n",
    "\n",
    "selected_columns = ['id','T1_ori_indexFrom0', 'category1','title','body','total_S_score','total_rank']\n",
    "merged_df_pure =  merged_df[selected_columns]\n",
    "\n",
    "total_result_pure_file_name = f\"./MergeFiveDScore/{model_CLS_name}/total_result_pure_{date_UNI}_{model_CLS_name}.csv\"\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "# merged_df_pure.to_csv('./MergeFiveDScore/total_result_pure.csv', index=False)\n",
    "merged_df_pure.to_csv(total_result_pure_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec0958-9052-427c-b0cd-f643db66c204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16e3b1-84c8-4baf-b874-f6d9e26b5495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
